\documentclass[12pt,a4paper]{article}

\usepackage{amsmath}
\usepackage[section]{algorithm}
\usepackage[numbered]{algo}
\usepackage{enumerate}

\begin{document}

\begin{algorithm}[h]
  \small
  \begin{enumerate}[(1)]
    \item For a sample $(x_n ,y^*_n)$, propagate the input $x_n$ through the
    network to compute the outputs $(v_{i_1}, \ldots, v_{i_{|V|}})$ (in topological order).
    \vspace{-6px}
    %\begin{enumerate}[(a)]
    %  \item Given a topological sort $V = (v_{i_1},\ldots,v_{i_{|V|}})$,
    %  sequentially compute the layers' outputs, also denoted by $v_{i_j}$.
    %  \item Then $y(x_n;w) = v_{i_{|V|}}$ is the network's output.
    %\end{enumerate}
    \item Compute the loss $\mathcal{L}_n := \mathcal{L}(v_{i_{|V|}}, y_n^*)$
    and its gradient
    \begin{align}
      \frac{\partial \mathcal{L}_n}{\partial v_{i_{|V|}}}.
    \end{align}
    \vspace{-6px}
    \item For each $j = |V|,\ldots,1$ compute
    \begin{align}
      \frac{\partial \mathcal{L}_n}{\partial w_j} =
      \frac{\partial \mathcal{L}_n}{\partial v_{i_{|V|}}} \prod_{k = j + 1}^{|V|} \frac{\partial v_{i_k}}{\partial v_{i_{k - 1}}}
      \frac{\partial v_{i_j}}{\partial w_j}.
    \end{align}
    where $w_j$ refers to the weights in node $i_j$.
    \vspace{-12px}
  \end{enumerate}
  \caption{Error backpropagation algorithm for a layered neural network
  represented as computation graph $G = (V,E)$.}
\end{algorithm}

\end{document}